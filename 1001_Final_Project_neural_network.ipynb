{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline \n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "from nltk.util import ngrams\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp = pd.read_csv('training_set_20k.csv', sep = '|')\n",
    "hp_test = pd.read_csv('test_set_2k.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_preprocessing(st):\n",
    "    \n",
    "    st = re.sub(r\"&quot;\", r\" \", st)\n",
    "    st = re.sub(r\"&apos;\", r\" \", st)\n",
    "    st = re.sub(r\"([.!?])\", r\" \", st)\n",
    "    st = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", st)\n",
    "    st = ' '.join(word for word in st.split(' ') if not word.startswith('httpwww'))\n",
    "    st = ' '.join(word for word in st.split(' ') if not word.startswith('OrderedD'))\n",
    "    st = st.replace('type external text','').replace('  ','').replace('In', ' In').replace('externaltype','')\n",
    "    st = st.replace('externalSource','').replace('type internal text','').replace('external Source','').replace('external external text Source','').replace('internal text','').replace('type internal','')\n",
    "    st = st.lower()\n",
    "    \n",
    "    return st    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the preprocessing on the 'Article' column\n",
    "hp['Article'] = hp['Article'].apply(article_preprocessing)\n",
    "hp_test['Article'] = hp_test['Article'].apply(article_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process the labels\n",
    "hp['Hyperpartisan'] = [0 if hp['Hyperpartisan'][i] == False else 1 for i in range(len(hp['Hyperpartisan']))]\n",
    "hp_test['Hyperpartisan'] = [0 if hp_test['Hyperpartisan'][i] == False else 1 for i in range(len(hp_test['Hyperpartisan']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 14000\n",
      "Val dataset size is 6000\n",
      "Test dataset size is 2000\n"
     ]
    }
   ],
   "source": [
    "# Split train data into actual train and validation sets 0.7/0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(hp, test_size=0.3, random_state=42)\n",
    "\n",
    "train_index = train_data['ID']\n",
    "train_article = train_data['Article']\n",
    "train_label = train_data['Hyperpartisan']\n",
    "\n",
    "val_index = val_data['ID']\n",
    "val_article = val_data['Article']\n",
    "val_label = val_data['Hyperpartisan']\n",
    "\n",
    "test_index = hp_test['ID']\n",
    "test_article = hp_test['Article']\n",
    "test_label = hp_test['Hyperpartisan']\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_index)))\n",
    "print (\"Val dataset size is {}\".format(len(val_index)))\n",
    "print (\"Test dataset size is {}\".format(len(test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean article length\n",
    "\n",
    "articles = train_article.values\n",
    "length_of_article = [len(article) for article in articles]\n",
    "MAX_LEN = np.mean(length_of_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detailed preprocesing, lemmatization, removing stopwords, punctuations and tokenization\n",
    "def preproc(train_article, n):\n",
    "    \n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for article in train_article.values:\n",
    "        article = nlp(article, disable=['tagger', 'parser', 'ner'])\n",
    "        tokenso = [tok.lemma_.strip() for tok in article if tok.lemma_ != '-PRON-']\n",
    "        \n",
    "        tokenso = [tok for tok in tokenso if tok not in stopwords and tok not in punctuations]\n",
    "     \n",
    "        # and get a list of all the n-grams,list of n-word tuples \n",
    "        grams = list(ngrams(tokenso, n))\n",
    "        \n",
    "        # token_dataset is a list of list of tuples\n",
    "        token_dataset.append(grams)\n",
    "        \n",
    "        all_tokens += grams\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize the training set\n",
    "train_article_token_dataset, train_all_tokens = preproc(train_article,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_article_token_dataset, val_all_tokens = preproc(val_article,1)\n",
    "test_article_token_dataset, test_all_tokens = preproc(test_article,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Maison',)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "max_vocab_size = 20000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = collections.Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(train_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20002\n",
      "20002\n"
     ]
    }
   ],
   "source": [
    "print(len(token2id))\n",
    "print(len(id2token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 3281\n",
      "Val dataset size is 547\n",
      "Test dataset size is 547\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "# get list of list of idxes\n",
    "train_data_indices = token2index_dataset(train_article_token_dataset)\n",
    "val_data_indices = token2index_dataset(val_article_token_dataset)\n",
    "test_data_indices = token2index_dataset(test_article_token_dataset)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(val_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_ARTICLE_LENGTH = MAX_LEN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, article_list, label_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of review tokens indexes\n",
    "        @param target_list: list of review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.article_list = article_list\n",
    "        self.label_list = label_list\n",
    "        assert (len(self.article_list) == len(self.label_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.article_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.article_list[key][:MAX_ARTICLE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def group_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    article_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        article_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(article_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "\n",
    "# batchsize better be power of 2\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = GroupDataset(train_data_indices, train_label.values.tolist())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=group_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = GroupDataset(val_data_indices, val_label.values.tolist())\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=group_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pdb\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH = MAX_LEN\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        # Binary calssification\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        # view basically reshapes it, so this averages it out. \n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate, scheduler=None):\n",
    "    acc_per_step_val = []\n",
    "    acc_per_step_train = []\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_per_epoch = []\n",
    "        acc_per_epoch_val = []\n",
    "        acc = []\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model) \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                acc.append(val_acc)\n",
    "                acc_per_epoch_val.append(val_acc)\n",
    "                acc_per_epoch.append(train_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "        #scheduler.step(loss)\n",
    "        print(\"Average accuracy is\"+ str(np.mean(acc)))\n",
    "        acc_per_step_val.append(acc_per_epoch_val)\n",
    "        acc_per_step_train.append(acc_per_epoch)\n",
    "    print(\"total average accuarcies validation\")\n",
    "    print(acc_per_step_val)\n",
    "    print(\"total accuracies train\")\n",
    "    print(acc_per_step_train)\n",
    "    save_model(model, acc_per_step_val, acc_per_step_train, \"Bag-of-words Deep Learning Model Performance on HyperPartisan Task\")\n",
    "    return acc_per_step_val, acc_per_step_train, model\n",
    "\n",
    "def save_model(model, val_accs, train_accs, title):\n",
    "    pdb.set_trace()\n",
    "    val_accs = np.array(val_accs)\n",
    "    max_val = val_accs.max()\n",
    "    train_accs = np.array(train_accs)\n",
    "    link =  \"\"\n",
    "    torch.save(model.state_dict(), link + \"model_states\")\n",
    "    pickle.dump(val_accs, open(link + \"val_accuracies\", \"wb\"))\n",
    "    pickle.dump(train_accs, open(link + \"train_accuracies\", \"wb\"))\n",
    "    pickle.dump(max_val, open(link + \"maxvalaccis\"+str(max_val), \"wb\"))\n",
    "    # this is when you want to overlay\n",
    "    num_in_epoch = np.shape(train_accs)[1]\n",
    "    num_epochs = np.shape(train_accs)[0]\n",
    "    x_vals = np.arange(0, num_epochs, 1.0/float(num_in_epoch))\n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(x_vals, train_accs.flatten(), label=\"Training Accuracy\")\n",
    "    plt.plot(x_vals, val_accs.flatten(), label=\"Validation Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylabel(\"Accuracy of Model With Given Parameter\")\n",
    "    plt.xlabel(\"Epochs (Batch Size 32)\")\n",
    "    plt.ylim(0,100)\n",
    "    plt.xlim(0, num_epochs)\n",
    "    plt.yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "    plt.xticks(np.arange(num_epochs + 1))\n",
    "    fig.savefig(link+\"graph.png\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "BATCH_SIZE = 32\n",
    "max_vocab_size = 20002\n",
    "# Criterion and Optimizer\n",
    "model = BagOfWords(max_vocab_size, 100)\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_data_indices = pickle.load(open('data/train_data_indexed', \"rb\"))\n",
    "val_data_indices = pickle.load(open('data/val_data_indexed', \"rb\"))\n",
    "\n",
    "train_labels = pd.read_pickle('data/train_labels').tolist()\n",
    "val_labels = pd.read_pickle('data/val_labels').tolist()\n",
    "\n",
    "\n",
    "convert_to_binary = {True: 1, False: 0}\n",
    "train_labeldf = [convert_to_binary[x] for x in train_labels]\n",
    "val_labeldf = [convert_to_binary[x] for x in val_labels]\n",
    "\n",
    "train_dataset = HyperPartGroupDataset(train_data_indices, train_labeldf)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=hype_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = HyperPartGroupDataset(val_data_indices, val_labeldf)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=hype_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "test_model_routine(train_loader, val_loader, model, criterion, optimizer, num_epochs, learning_rate)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_encoder(nn.Module):\n",
    "    def __init__(self, weight, hidden_size, num_layers, fc_hidden_size, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # weight: pretrained weight matrix\n",
    "        # hidden_size: Hidden Size of layer in the RNN encoder\n",
    "        # num_layers: number of layers in RNN\n",
    "        # fc_hidden_size: hidden size of fully connected layer\n",
    "        # num_classes: number of output classes\n",
    "        \n",
    "        super(RNN_encoder, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_size, num_layers, batch_first=True, bidirectional = True) # the batch is actaully in a descending order\n",
    "        self.fc_model_concat = nn.Sequential(\n",
    "            nn.Linear(hidden_size, fc_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(fc_hidden_size, num_classes)\n",
    "                )\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0, it is actually h0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        # this step is used to initialize the initial hidden state, for each sentence\n",
    "        # because this evolves differently with each sentence, it can be initilized for each sentence\n",
    "        k = self.num_layers*2\n",
    "        hidden = torch.randn(k, batch_size, self.hidden_size)\n",
    "\n",
    "        return Variable(hidden)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = data.size()\n",
    "        hidden = self.init_hidden(batch_size).to(device)\n",
    "        sorted_lengths, idx_sort = torch.sort(length, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        idx_sort = Variable(idx_sort)\n",
    "        idx_unsort = Variable(idx_unsort).to(device)\n",
    "        data = data.index_select(0, idx_sort1).to(device)\n",
    "              \n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(data)\n",
    "        # pack padded sequence\n",
    "        # input: batch*length*dim\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, sorted_lengths.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        # input: batch seq feature\n",
    "        _, h_n = self.rnn(embed,hidden)\n",
    "        h_n.to(device)\n",
    "        h_n = h_n.index_select(1, idx_unsort)\n",
    "\n",
    "        encoded_out = torch.sum(h_n, dim=0)\n",
    "        \n",
    "        \n",
    "        out = self.fc_model_concat(encoded_out.float())\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_rnn(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    model.eval()   ### this is the testing part\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data, lengths, labels in loader: \n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            outputs = F.softmax(model(data_batch,length_batch ), dim=1) ### after softmax, it becomes probability\n",
    "            predicted = outputs.max(1, keepdim=True)[1].to(device)\n",
    "\n",
    "            total += label_batch.size(0)\n",
    "            correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "            val_acc = 100 * correct / total\n",
    "            \n",
    "            val_loss_s = rnn_criterion(outputs, label_batch)\n",
    "            val_loss_s = val_loss_s.item() * label_batch.size(0) \n",
    "            \n",
    "            val_loss += val_loss_s / len(loader.dataset)\n",
    "            \n",
    "    rnn_optimizer.zero_grad() \n",
    "    \n",
    "     \n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "model_rnn = RNN_encoder(weight, 450, 1, 300,3)\n",
    "\n",
    "# print the number of trained parameters in the model\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model_rnn.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "      \n",
    "rnn_learning_rate = 0.001\n",
    "rnn_num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "rnn_criterion = torch.nn.CrossEntropyLoss()\n",
    "rnn_optimizer = torch.optim.Adam(model_rnn.parameters(), lr=rnn_learning_rate)\n",
    "\n",
    "# Train the model\n",
    "rnn_total_step = len(snli_train_loader)\n",
    "\n",
    "rnn_training_loss = []\n",
    "rnn_training_acc = []\n",
    "rnn_validation_loss = []\n",
    "rnn_validation_acc = []\n",
    "            \n",
    "for epoch in range(rnn_num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        \n",
    "        rnn_train_total = 0\n",
    "        rnn_train_correct = 0\n",
    "        model_rnn.to(device)\n",
    "        model_rnn.train()\n",
    "        rnn_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        rnn_outputs = model_rnn(data_batch, length_batch).to(device)\n",
    "       \n",
    "      \n",
    "        # calculate the training loss and append it\n",
    "        rnn_training_loss_s = rnn_criterion(rnn_outputs, label_batch).to(device)\n",
    "        \n",
    "        \n",
    "        # Backward and optimize\n",
    "        rnn_training_loss_s.backward()\n",
    "        rnn_optimizer.step()\n",
    "        \n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model_rnn(val_loader, model_rnn)\n",
    "            \n",
    "            rnn_validation_loss.append(val_loss)\n",
    "            rnn_validation_acc.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {},Validation loss: {}'.format(\n",
    "                       epoch+1, rnn_num_epochs, i+1, len(_train_loader), val_acc, val_loss))\n",
    "\n",
    "            \n",
    "    tra_acc, tra_loss = test_model_rnn(train_loader, model_rnn)    \n",
    "    rnn_training_loss.append(tra_loss)\n",
    "    rnn_training_acc.append(tra_acc)\n",
    "    print('Epoch:{},Training Acc: {}, Training loss: {}'.format(epoch, tra_acc, tra_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
